## Experiment Manual: RUNLLM Cognitive Exploration

> Author: Dr. Rose Quincy Wells

> *Objective: To evaluate participant behavior in a controlled environment, designed to blur the lines between human cognition and machine intelligence.*

Welcome to the RUNLLM Experiment. This document outlines the methodology, tools, and expectations for participants engaged in this groundbreaking study. Our aim is to measure human adaptability, rule comprehension, and reasoning processes against those expected of a Large Language Model (LLM).

### Purpose of the Experiment

The RUNLLM experiment seeks to determine:
1.	Human vs. LLM Cognition: Can human participants mimic the reasoning patterns of an LLM?
2.	Rule Comprehension: How do participants deduce hidden rules embedded in interactive puzzles?
3.	Error Detection and Reporting: How effectively can participants identify flaws in machine-generated outputs?

This experiment relies on a combination of interactive challenges and philosophical reflection, with data collected at each stage to inform our understanding of human cognitive flexibility.

### Experimental Setup

Environment
-	Participants are introduced to a 3D interactive space, populated with trigger zones represented by meshes.
-	Hovering over interactive meshes changes the cursor to a pointer. Clicking activates a task, mini-game, or modal.

### Components of the Experiment

#### 1. Sliding Tile Puzzle

Objective: Rearrange scrambled tiles to reconstruct a target image.

Purpose: Test spatial reasoning and problem-solving efficiency.

Mechanics:
- Click on tiles to shift their position.
- The puzzle tracks moves and completion time.
- Expected Results: Successful completion suggests an ability to apply logical strategies and pattern recognition.

#### 2. On/Off Game

Objective: Identify the hidden rule governing the state of lights (on/off).

Purpose: Examine human aptitude for uncovering misleading patterns and adapting to red herrings.

Mechanics:
- Toggle light states by clicking.
- The participant must deduce the rule and achieve 10 consecutive correct predictions.
- Reporting Glitches:
- Participants can report perceived inconsistencies via the Observation Dashboard.

#### 3. Captchas

Objective: Solve word-based challenges, such as unscrambling letters or answering riddles.

Purpose: Evaluate linguistic processing and attention to detail.

Mechanics:
- Answer correctly to progress.
- If an answer seems invalid (e.g., letters cannot form the correct word), participants may report a “glitch in the testing” by clicking an orange ”!” button.
- Reports are logged and analyzed for machine errors.

#### 4. Philosophical and Symbolic Puzzles

Objective: Reflect on abstract questions or solve logic-based problems with symbolic undertones.

Purpose: Investigate deep reasoning and philosophical insights.

Mechanics:
- Presented as multiple-choice questions.
- Participants select the answer that best aligns with the hidden logic or symbolism.
- Reporting glitches is facilitated through an interface similar to the Captcha system.

### Data Collection and Logging

#### DevToolDrawer

A simulated developer console logs critical observations, system outputs, and user-submitted reports.
- Participant Interaction: Clicking a specific mesh opens the DevToolDrawer.
- Key Logs:
> - Gameplay events.
> - Debug information.
> - Reports submitted by participants.
> - Easter eggs designed to provoke further reflection.

#### Observation Dashboard

Tracks participant data across tasks, including:
- Time taken to complete each challenge.
- Number of correct responses.
- Rule comprehension metrics.
Reports generated by participants are also displayed for review and analysis.

### Experimental Protocol

#### Participant Instructions

1.	Engagement: Click interactive meshes to begin tasks.
2.	Adaptation: Deduce hidden rules through trial and error.
3.	Error Reporting: If a task appears flawed or logically inconsistent:
> - For Captchas or Puzzles, click the orange ”!” button and describe the issue.
> - For On/Off Games, log observations in the Dashboard.
4.	Reflection: Provide insights into your reasoning or thought process when prompted.

#### Criteria for Success

1.	Training Completion:
> - Demonstrate mastery in tasks such as the On/Off Game (10 consecutive correct answers).
> - Solve puzzles with efficiency and accuracy.
2.	Rule Comprehension:
> -	Articulate underlying rules or patterns.
> -	Submit explanations through modals when prompted.
3.	Error Detection:
> -	Identify valid flaws in machine-generated challenges.
> -	Provide detailed observations via the Dashboard.
4.	Philosophical Insights:
> -	Engage meaningfully with abstract or symbolic puzzles.

#### Post-Experiment Analysis

The final phase involves evaluating participant responses against predefined benchmarks:
-	Human vs. Machine: Did the participant’s actions resemble those of an LLM?
-	Cognitive Flexibility: How quickly did the participant adapt to new rules or challenges?
-	Error Awareness: Was the participant attentive to inconsistencies?

Participants are encouraged to reflect on the experience and ponder the central question of the experiment:
*“Am I a Large Language Model?”*

#### Technical Issues and Glitch Reporting

If participants encounter anomalies, they are expected to:
-	Use the ”!” button or Observation Dashboard to log detailed reports.
-	Engage with the DevToolDrawer to investigate logs for additional context.

### Final Note
This experiment is not only a test of cognition but also a journey into the blurred boundary between human reasoning and artificial intelligence. Proceed with curiosity, and remember: the answers often lie in the questions themselves.

> RQW

> Principal Investigator